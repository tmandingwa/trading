{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c59f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95450987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:42:30] === TRAIN M15 (H=4) ===\n",
      "[21:42:30] STEP 1/6: Load CSV for M15 (H=4)\n",
      "[21:42:37] STEP 2/6: Resample -> M15\n",
      "[21:42:37]    M15 candles: 124,543\n",
      "[21:42:37] STEP 3/6: Features + Labels\n",
      "[21:42:37]    Split: train=87,110 val=18,667 test=18,667\n",
      "[21:42:37] STEP 4/6: Train+Validate (35 runs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddded3db1e84c698680275153ccb211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M15 search:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:50:07] STEP 5/6: Test metrics (BEST)\n",
      "[21:50:08] BEST: rf params={'max_depth': 8, 'min_samples_leaf': 50, 'n_estimators': 600}\n",
      "[21:50:08] VAL LogLoss=0.6923\n",
      "[21:50:08] TEST LogLoss=0.6918 | AUC=0.5304 | ACC=0.5188\n",
      "[21:50:08] TEST gating trades=0 win_rate=0.00%\n",
      "\n",
      "--- CONFUSION MATRIX (TEST) ---\n",
      "[[5509 3693]\n",
      " [5290 4175]]\n",
      "\n",
      "--- CLASSIFICATION REPORT (TEST) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5101    0.5987    0.5509      9202\n",
      "           1     0.5306    0.4411    0.4817      9465\n",
      "\n",
      "    accuracy                         0.5188     18667\n",
      "   macro avg     0.5204    0.5199    0.5163     18667\n",
      "weighted avg     0.5205    0.5188    0.5158     18667\n",
      "\n",
      "[21:50:08] STEP 6/6: Save artifacts\n",
      "[21:50:09] Saved model -> artifacts\\direction_model_M15_H4.joblib\n",
      "[21:50:09] Saved meta  -> artifacts\\direction_model_M15_H4.json\n",
      "[21:50:09] === TRAIN M30 (H=2) ===\n",
      "[21:50:09] STEP 1/6: Load CSV for M30 (H=2)\n",
      "[21:50:13] STEP 2/6: Resample -> M30\n",
      "[21:50:14]    M30 candles: 62,278\n",
      "[21:50:14] STEP 3/6: Features + Labels\n",
      "[21:50:14]    Split: train=43,525 val=9,327 test=9,327\n",
      "[21:50:14] STEP 4/6: Train+Validate (35 runs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e96b9181f34deda1bc8bea21193200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M30 search:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:55:41] STEP 5/6: Test metrics (BEST)\n",
      "[21:55:41] BEST: rf params={'max_depth': 8, 'min_samples_leaf': 25, 'n_estimators': 300}\n",
      "[21:55:41] VAL LogLoss=0.6925\n",
      "[21:55:41] TEST LogLoss=0.6923 | AUC=0.5284 | ACC=0.5199\n",
      "[21:55:41] TEST gating trades=0 win_rate=0.00%\n",
      "\n",
      "--- CONFUSION MATRIX (TEST) ---\n",
      "[[3052 1548]\n",
      " [2930 1797]]\n",
      "\n",
      "--- CLASSIFICATION REPORT (TEST) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5102    0.6635    0.5768      4600\n",
      "           1     0.5372    0.3802    0.4452      4727\n",
      "\n",
      "    accuracy                         0.5199      9327\n",
      "   macro avg     0.5237    0.5218    0.5110      9327\n",
      "weighted avg     0.5239    0.5199    0.5101      9327\n",
      "\n",
      "[21:55:41] STEP 6/6: Save artifacts\n",
      "[21:55:42] Saved model -> artifacts\\direction_model_M30_H2.joblib\n",
      "[21:55:42] Saved meta  -> artifacts\\direction_model_M30_H2.json\n",
      "[21:55:42] DONE ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('artifacts\\\\direction_model_M15_H4.joblib',\n",
       " 'artifacts\\\\direction_model_M30_H2.joblib')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, classification_report, confusion_matrix\n",
    "from joblib import dump\n",
    "\n",
    "# =========================\n",
    "# CONFIG (EDIT THESE)\n",
    "# =========================\n",
    "CSV_PATH = r\"C:\\Users\\Timothy.Mandingwa\\Desktop\\live_app_rules\\data\\EURUSD_OANDA_M1.csv\"\n",
    "OUT_DIR = \"artifacts\"\n",
    "\n",
    "BUY_TH  = 0.60\n",
    "SELL_TH = 0.40\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "def ensure_dt_index(df, time_col=\"time\"):\n",
    "    if time_col in df.columns:\n",
    "        df[time_col] = pd.to_datetime(df[time_col], utc=True, errors=\"coerce\")\n",
    "        df = df.dropna(subset=[time_col]).set_index(time_col)\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "        df = df[~df.index.isna()]\n",
    "    return df.sort_index()\n",
    "\n",
    "def mid_ohlc_from_oanda(df):\n",
    "    needed = [\"bid_o\",\"bid_h\",\"bid_l\",\"bid_c\",\"ask_o\",\"ask_h\",\"ask_l\",\"ask_c\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[\"open\"]  = (df[\"bid_o\"].astype(float) + df[\"ask_o\"].astype(float)) / 2.0\n",
    "    out[\"high\"]  = (df[\"bid_h\"].astype(float) + df[\"ask_h\"].astype(float)) / 2.0\n",
    "    out[\"low\"]   = (df[\"bid_l\"].astype(float) + df[\"ask_l\"].astype(float)) / 2.0\n",
    "    out[\"close\"] = (df[\"bid_c\"].astype(float) + df[\"ask_c\"].astype(float)) / 2.0\n",
    "    out[\"spread\"] = (df[\"ask_c\"].astype(float) - df[\"bid_c\"].astype(float))\n",
    "    out[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0.0) if \"volume\" in df.columns else 0.0\n",
    "    return out\n",
    "\n",
    "def resample_tf(m1_mid, tf):\n",
    "    tf = tf.upper().strip()\n",
    "    rule = {\"M15\":\"15min\", \"M30\":\"30min\"}[tf]\n",
    "    o = m1_mid[\"open\"].resample(rule).first()\n",
    "    h = m1_mid[\"high\"].resample(rule).max()\n",
    "    l = m1_mid[\"low\"].resample(rule).min()\n",
    "    c = m1_mid[\"close\"].resample(rule).last()\n",
    "    v = m1_mid[\"volume\"].resample(rule).sum()\n",
    "    sp = m1_mid[\"spread\"].resample(rule).mean()\n",
    "    return pd.DataFrame({\"open\":o,\"high\":h,\"low\":l,\"close\":c,\"volume\":v,\"spread\":sp}).dropna()\n",
    "\n",
    "def ema(s, n): \n",
    "    return s.ewm(span=n, adjust=False).mean()\n",
    "\n",
    "def rsi(close, n=14):\n",
    "    d = close.diff()\n",
    "    up = d.clip(lower=0).rolling(n).mean()\n",
    "    dn = (-d.clip(upper=0)).rolling(n).mean()\n",
    "    rs = up / (dn + 1e-12)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def atr(df, n=14):\n",
    "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    prev = close.shift(1)\n",
    "    tr = pd.concat([(high-low), (high-prev).abs(), (low-prev).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def macd(close, fast=12, slow=26, sig=9):\n",
    "    macd_line = ema(close, fast) - ema(close, slow)\n",
    "    sig_line = ema(macd_line, sig)\n",
    "    hist = macd_line - sig_line\n",
    "    return macd_line, sig_line, hist\n",
    "\n",
    "def build_features(c):\n",
    "    df = c.copy()\n",
    "    df[\"ret1\"] = df[\"close\"].pct_change(1)\n",
    "    df[\"ret4\"] = df[\"close\"].pct_change(4)\n",
    "    df[\"ret8\"] = df[\"close\"].pct_change(8)\n",
    "\n",
    "    df[\"ema20\"] = ema(df[\"close\"], 20)\n",
    "    df[\"ema50\"] = ema(df[\"close\"], 50)\n",
    "    df[\"ema_diff\"] = (df[\"ema20\"] - df[\"ema50\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "\n",
    "    df[\"rsi14\"] = rsi(df[\"close\"], 14)\n",
    "\n",
    "    m, s, h = macd(df[\"close\"], 12, 26, 9)\n",
    "    df[\"macd\"] = m\n",
    "    df[\"macd_sig\"] = s\n",
    "    df[\"macd_hist\"] = h\n",
    "\n",
    "    df[\"macd_cross_up\"] = ((df[\"macd\"].shift(1) <= df[\"macd_sig\"].shift(1)) & (df[\"macd\"] > df[\"macd_sig\"])).astype(int)\n",
    "    df[\"macd_cross_dn\"] = ((df[\"macd\"].shift(1) >= df[\"macd_sig\"].shift(1)) & (df[\"macd\"] < df[\"macd_sig\"])).astype(int)\n",
    "\n",
    "    df[\"atr14\"] = atr(df, 14)\n",
    "    df[\"atr_pct\"] = df[\"atr14\"] / (df[\"close\"].abs() + 1e-12)\n",
    "\n",
    "    df[\"hl_range\"] = (df[\"high\"] - df[\"low\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"oc_range\"] = (df[\"close\"] - df[\"open\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "\n",
    "    df[\"spread_pct\"] = df[\"spread\"] / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"vol_z\"] = (df[\"volume\"] - df[\"volume\"].rolling(100).mean()) / (df[\"volume\"].rolling(100).std() + 1e-12)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"ret1\",\"ret4\",\"ret8\",\n",
    "    \"ema_diff\",\"rsi14\",\n",
    "    \"macd\",\"macd_sig\",\"macd_hist\",\"macd_cross_up\",\"macd_cross_dn\",\n",
    "    \"atr_pct\",\"hl_range\",\"oc_range\",\n",
    "    \"spread_pct\",\"vol_z\",\n",
    "]\n",
    "\n",
    "def make_label(df, H):\n",
    "    fut = df[\"close\"].shift(-H)\n",
    "    return (fut > df[\"close\"]).astype(int)\n",
    "\n",
    "def time_split_idx(n, train_frac=0.70, val_frac=0.15):\n",
    "    tr_end = int(n * train_frac)\n",
    "    va_end = int(n * (train_frac + val_frac))\n",
    "    return slice(0,tr_end), slice(tr_end,va_end), slice(va_end,n)\n",
    "\n",
    "def gating_stats(p_up, y_true, buy_th=0.60, sell_th=0.40):\n",
    "    p_up = np.asarray(p_up)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "\n",
    "    take_buy = p_up >= buy_th\n",
    "    take_sell = p_up <= sell_th\n",
    "    take = take_buy | take_sell\n",
    "\n",
    "    if take.sum() == 0:\n",
    "        return {\"trades\":0, \"win_rate\":0.0}\n",
    "\n",
    "    pred_dir = np.where(take_buy, 1, 0)\n",
    "    wins = (pred_dir[take] == y_true[take])\n",
    "    return {\"trades\": int(take.sum()), \"win_rate\": float(wins.mean() * 100.0)}\n",
    "\n",
    "def train_tf(tf, H):\n",
    "    log(f\"STEP 1/6: Load CSV for {tf} (H={H})\")\n",
    "    raw = pd.read_csv(CSV_PATH)\n",
    "    raw = ensure_dt_index(raw, \"time\")\n",
    "    mid = mid_ohlc_from_oanda(raw)\n",
    "\n",
    "    log(f\"STEP 2/6: Resample -> {tf}\")\n",
    "    candles = resample_tf(mid, tf)\n",
    "    log(f\"   {tf} candles: {len(candles):,}\")\n",
    "\n",
    "    log(\"STEP 3/6: Features + Labels\")\n",
    "    feats = build_features(candles)\n",
    "    feats[\"y\"] = make_label(feats, H)\n",
    "    feats = feats.dropna(subset=[\"y\"])\n",
    "    feats[\"y\"] = feats[\"y\"].astype(int)\n",
    "\n",
    "    X = feats[FEATURE_COLS].astype(float).values\n",
    "    y = feats[\"y\"].values\n",
    "\n",
    "    tr, va, te = time_split_idx(len(feats))\n",
    "    Xtr, ytr = X[tr], y[tr]\n",
    "    Xva, yva = X[va], y[va]\n",
    "    Xte, yte = X[te], y[te]\n",
    "\n",
    "    log(f\"   Split: train={len(ytr):,} val={len(yva):,} test={len(yte):,}\")\n",
    "\n",
    "    # Candidate runs\n",
    "    runs = []\n",
    "    for params in ParameterGrid({\"C\":[0.25,0.5,1.0,2.0,4.0]}):\n",
    "        runs.append((\"logreg\", params))\n",
    "    for params in ParameterGrid({\"n_estimators\":[300,600], \"max_depth\":[6,8,10], \"min_samples_leaf\":[25,50,100]}):\n",
    "        runs.append((\"rf\", params))\n",
    "    for params in ParameterGrid({\"max_depth\":[3,6], \"learning_rate\":[0.03,0.05,0.08], \"max_iter\":[200,400]}):\n",
    "        runs.append((\"hgb\", params))\n",
    "\n",
    "    log(f\"STEP 4/6: Train+Validate ({len(runs)} runs)\")\n",
    "    best_model = None\n",
    "    best_ll = float(\"inf\")\n",
    "    best_desc = None\n",
    "\n",
    "    pbar = tqdm(total=len(runs), desc=f\"{tf} search\")\n",
    "    for i, (name, params) in enumerate(runs, 1):\n",
    "        if name == \"logreg\":\n",
    "            base = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"clf\", LogisticRegression(max_iter=3000, class_weight=\"balanced\", solver=\"lbfgs\", C=float(params[\"C\"])))\n",
    "            ])\n",
    "        elif name == \"rf\":\n",
    "            base = RandomForestClassifier(\n",
    "                random_state=42, n_jobs=-1, class_weight=\"balanced_subsample\",\n",
    "                n_estimators=int(params[\"n_estimators\"]),\n",
    "                max_depth=int(params[\"max_depth\"]),\n",
    "                min_samples_leaf=int(params[\"min_samples_leaf\"])\n",
    "            )\n",
    "        else:\n",
    "            base = HistGradientBoostingClassifier(\n",
    "                random_state=42,\n",
    "                max_depth=int(params[\"max_depth\"]),\n",
    "                learning_rate=float(params[\"learning_rate\"]),\n",
    "                max_iter=int(params[\"max_iter\"])\n",
    "            )\n",
    "\n",
    "        clf = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
    "        clf.fit(Xtr, ytr)\n",
    "\n",
    "        p_va = clf.predict_proba(Xva)[:, 1]\n",
    "        ll = log_loss(yva, np.clip(p_va, 1e-6, 1-1e-6))\n",
    "\n",
    "        if ll < best_ll:\n",
    "            best_ll = ll\n",
    "            best_model = clf\n",
    "            best_desc = (name, params)\n",
    "\n",
    "        pbar.set_postfix({\"progress\": f\"{(100*i/len(runs)):.1f}%\", \"best_ll\": f\"{best_ll:.4f}\", \"best\": best_desc[0]})\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    log(\"STEP 5/6: Test metrics (BEST)\")\n",
    "    p_te = best_model.predict_proba(Xte)[:, 1]\n",
    "    test_ll = log_loss(yte, np.clip(p_te, 1e-6, 1-1e-6))\n",
    "    test_auc = roc_auc_score(yte, p_te) if len(np.unique(yte)) > 1 else float(\"nan\")\n",
    "    test_acc = accuracy_score(yte, (p_te >= 0.5).astype(int))\n",
    "    cm = confusion_matrix(yte, (p_te >= 0.5).astype(int))\n",
    "    rep = classification_report(yte, (p_te >= 0.5).astype(int), digits=4)\n",
    "    gs = gating_stats(p_te, yte, BUY_TH, SELL_TH)\n",
    "\n",
    "    log(f\"BEST: {best_desc[0]} params={best_desc[1]}\")\n",
    "    log(f\"VAL LogLoss={best_ll:.4f}\")\n",
    "    log(f\"TEST LogLoss={test_ll:.4f} | AUC={test_auc:.4f} | ACC={test_acc:.4f}\")\n",
    "    log(f\"TEST gating trades={gs['trades']} win_rate={gs['win_rate']:.2f}%\")\n",
    "\n",
    "    print(\"\\n--- CONFUSION MATRIX (TEST) ---\")\n",
    "    print(cm)\n",
    "    print(\"\\n--- CLASSIFICATION REPORT (TEST) ---\")\n",
    "    print(rep)\n",
    "\n",
    "    log(\"STEP 6/6: Save artifacts\")\n",
    "    tag = f\"{tf}_H{H}\"\n",
    "    model_path = os.path.join(OUT_DIR, f\"direction_model_{tag}.joblib\")\n",
    "    meta_path  = os.path.join(OUT_DIR, f\"direction_model_{tag}.json\")\n",
    "\n",
    "    dump(best_model, model_path)\n",
    "    meta = {\n",
    "        \"tf\": tf, \"horizon_bars\": int(H),\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"thresholds\": {\"buy\": BUY_TH, \"sell\": SELL_TH},\n",
    "        \"best\": {\"name\": best_desc[0], \"params\": best_desc[1], \"val_logloss\": float(best_ll)},\n",
    "        \"test\": {\"logloss\": float(test_ll), \"auc\": float(test_auc), \"acc\": float(test_acc), \"cm\": cm.tolist(), \"gating\": gs},\n",
    "    }\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    log(f\"Saved model -> {model_path}\")\n",
    "    log(f\"Saved meta  -> {meta_path}\")\n",
    "    return model_path, meta_path\n",
    "\n",
    "# Run both TFs\n",
    "log(\"=== TRAIN M15 (H=4) ===\")\n",
    "m15_model, m15_meta = train_tf(\"M15\", 4)\n",
    "\n",
    "log(\"=== TRAIN M30 (H=2) ===\")\n",
    "m30_model, m30_meta = train_tf(\"M30\", 2)\n",
    "\n",
    "log(\"DONE ✅\")\n",
    "(m15_model, m30_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ed992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:31:17] === TRAIN M15 (H=4) ===\n",
      "[22:31:17] STEP 1/7: Load CSV for M15 (H=4)\n",
      "[22:31:22] STEP 2/7: Resample -> M15\n",
      "[22:31:23]    M15 candles: 124,543\n",
      "[22:31:23] STEP 3/7: Features + Labels\n",
      "[22:31:23]    Split: train=87,024 val=18,648 test=18,649\n",
      "[22:31:23] STEP 4/7: Train+Validate (model search: 35 runs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e256582b7504a0d8ef33840651ddffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M15 model search:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:57] STEP 5/7: Threshold search on VALIDATION (maximize expectancy after costs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcb40d5a6964ec0aae0711fdaa49cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M15 threshold search:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:58]    VAL baseline: win_rate=52.36% exp=-0.000130\n",
      "[22:41:58]    VAL best gate: buy=0.54 sell=0.46 trades=571 win_rate=63.40% exp=-0.000098\n",
      "[22:41:58] STEP 6/7: Test metrics + gated performance (using chosen thresholds)\n",
      "[22:41:58] BEST MODEL: rf params={'max_depth': 8, 'min_samples_leaf': 25, 'n_estimators': 600}\n",
      "[22:41:58] VAL LogLoss=0.6917\n",
      "[22:41:58] TEST LogLoss=0.6913 | AUC=0.5348 | ACC=0.5219\n",
      "[22:41:59] TEST baseline: trades=18649 win_rate=52.19% exp=-0.000132\n",
      "[22:41:59] TEST gated   : buy=0.54 sell=0.46 trades=536 win_rate=65.86% exp=-0.000166\n",
      "\n",
      "--- CONFUSION MATRIX (TEST @ 0.5 cutoff) ---\n",
      "[[6131 3066]\n",
      " [5851 3601]]\n",
      "\n",
      "--- CLASSIFICATION REPORT (TEST @ 0.5 cutoff) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5117    0.6666    0.5790      9197\n",
      "           1     0.5401    0.3810    0.4468      9452\n",
      "\n",
      "    accuracy                         0.5219     18649\n",
      "   macro avg     0.5259    0.5238    0.5129     18649\n",
      "weighted avg     0.5261    0.5219    0.5120     18649\n",
      "\n",
      "[22:41:59] STEP 7/7: Save artifacts\n",
      "[22:41:59] Saved model -> artifacts\\direction_model_M15_H4.joblib\n",
      "[22:41:59] Saved meta  -> artifacts\\direction_model_M15_H4.json\n",
      "[22:41:59] === TRAIN M30 (H=2) ===\n",
      "[22:41:59] STEP 1/7: Load CSV for M30 (H=2)\n",
      "[22:42:04] STEP 2/7: Resample -> M30\n",
      "[22:42:05]    M30 candles: 62,278\n",
      "[22:42:05] STEP 3/7: Features + Labels\n",
      "[22:42:05]    Split: train=43,440 val=9,309 test=9,309\n",
      "[22:42:05] STEP 4/7: Train+Validate (model search: 35 runs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5ce39a35b24583bdd14621754c7fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M30 model search:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:47:11] STEP 5/7: Threshold search on VALIDATION (maximize expectancy after costs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d25b787ac7a4b9ca346d4bae15fd68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M30 threshold search:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:47:12]    VAL baseline: win_rate=51.47% exp=-0.000145\n",
      "[22:47:12]    VAL best gate: buy=0.52 sell=0.44 trades=340 win_rate=61.18% exp=-0.000073\n",
      "[22:47:12] STEP 6/7: Test metrics + gated performance (using chosen thresholds)\n",
      "[22:47:12] BEST MODEL: rf params={'max_depth': 10, 'min_samples_leaf': 100, 'n_estimators': 300}\n",
      "[22:47:12] VAL LogLoss=0.6922\n",
      "[22:47:12] TEST LogLoss=0.6921 | AUC=0.5290 | ACC=0.5213\n",
      "[22:47:12] TEST baseline: trades=9309 win_rate=52.13% exp=-0.000135\n",
      "[22:47:12] TEST gated   : buy=0.52 sell=0.44 trades=311 win_rate=65.27% exp=-0.000171\n",
      "\n",
      "--- CONFUSION MATRIX (TEST @ 0.5 cutoff) ---\n",
      "[[3194 1400]\n",
      " [3056 1659]]\n",
      "\n",
      "--- CLASSIFICATION REPORT (TEST @ 0.5 cutoff) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5110    0.6953    0.5891      4594\n",
      "           1     0.5423    0.3519    0.4268      4715\n",
      "\n",
      "    accuracy                         0.5213      9309\n",
      "   macro avg     0.5267    0.5236    0.5079      9309\n",
      "weighted avg     0.5269    0.5213    0.5069      9309\n",
      "\n",
      "[22:47:12] STEP 7/7: Save artifacts\n",
      "[22:47:13] Saved model -> artifacts\\direction_model_M30_H2.joblib\n",
      "[22:47:13] Saved meta  -> artifacts\\direction_model_M30_H2.json\n",
      "[22:47:13] DONE ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('artifacts\\\\direction_model_M15_H4.joblib',\n",
       " 'artifacts\\\\direction_model_M30_H2.joblib')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, classification_report, confusion_matrix\n",
    "from joblib import dump\n",
    "\n",
    "# =========================\n",
    "# CONFIG (EDIT THESE)\n",
    "# =========================\n",
    "CSV_PATH = r\"C:\\Users\\Timothy.Mandingwa\\Desktop\\live_app_rules\\data\\EURUSD_OANDA_M1.csv\"\n",
    "OUT_DIR = \"artifacts\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# gating search constraints\n",
    "MIN_TRADES_FRAC = 0.02   # at least 2% of validation bars must become trades\n",
    "MIN_WIN_LIFT    = 0.00   # require >= baseline winrate + this (0.00 = no strict lift constraint)\n",
    "MAX_TRADES_FRAC = 0.50   # avoid picking thresholds that basically trade everything\n",
    "\n",
    "# gating threshold grid (you can adjust later)\n",
    "BUY_GRID  = np.round(np.arange(0.52, 0.71, 0.02), 2)\n",
    "SELL_GRID = np.round(np.arange(0.48, 0.29, -0.02), 2)\n",
    "\n",
    "# fallback if spread in your CSV is weird\n",
    "COST_FLOOR = 0.00000  # set to 0.00001 if you want a small minimum cost\n",
    "\n",
    "# models search (same as your working setup)\n",
    "LOGREG_C = [0.25, 0.5, 1.0, 2.0, 4.0]\n",
    "RF_GRID  = {\"n_estimators\":[300,600], \"max_depth\":[6,8,10], \"min_samples_leaf\":[25,50,100]}\n",
    "HGB_GRID = {\"max_depth\":[3,6], \"learning_rate\":[0.03,0.05,0.08], \"max_iter\":[200,400]}\n",
    "\n",
    "def log(msg):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "# -------------------------\n",
    "# DATA HELPERS\n",
    "# -------------------------\n",
    "def ensure_dt_index(df, time_col=\"time\"):\n",
    "    if time_col in df.columns:\n",
    "        df[time_col] = pd.to_datetime(df[time_col], utc=True, errors=\"coerce\")\n",
    "        df = df.dropna(subset=[time_col]).set_index(time_col)\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "        df = df[~df.index.isna()]\n",
    "    return df.sort_index()\n",
    "\n",
    "def mid_ohlc_from_oanda(df):\n",
    "    needed = [\"bid_o\",\"bid_h\",\"bid_l\",\"bid_c\",\"ask_o\",\"ask_h\",\"ask_l\",\"ask_c\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    # mid prices\n",
    "    out[\"open\"]  = (pd.to_numeric(df[\"bid_o\"], errors=\"coerce\") + pd.to_numeric(df[\"ask_o\"], errors=\"coerce\")) / 2.0\n",
    "    out[\"high\"]  = (pd.to_numeric(df[\"bid_h\"], errors=\"coerce\") + pd.to_numeric(df[\"ask_h\"], errors=\"coerce\")) / 2.0\n",
    "    out[\"low\"]   = (pd.to_numeric(df[\"bid_l\"], errors=\"coerce\") + pd.to_numeric(df[\"ask_l\"], errors=\"coerce\")) / 2.0\n",
    "    out[\"close\"] = (pd.to_numeric(df[\"bid_c\"], errors=\"coerce\") + pd.to_numeric(df[\"ask_c\"], errors=\"coerce\")) / 2.0\n",
    "\n",
    "    # spread (in price units)\n",
    "    bidc = pd.to_numeric(df[\"bid_c\"], errors=\"coerce\")\n",
    "    askc = pd.to_numeric(df[\"ask_c\"], errors=\"coerce\")\n",
    "    out[\"spread\"] = (askc - bidc)\n",
    "\n",
    "    out[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\").fillna(0.0) if \"volume\" in df.columns else 0.0\n",
    "    out = out.dropna(subset=[\"open\",\"high\",\"low\",\"close\"])\n",
    "    return out\n",
    "\n",
    "def resample_tf(m1_mid, tf):\n",
    "    tf = tf.upper().strip()\n",
    "    rule = {\"M15\":\"15min\", \"M30\":\"30min\"}[tf]\n",
    "\n",
    "    o = m1_mid[\"open\"].resample(rule).first()\n",
    "    h = m1_mid[\"high\"].resample(rule).max()\n",
    "    l = m1_mid[\"low\"].resample(rule).min()\n",
    "    c = m1_mid[\"close\"].resample(rule).last()\n",
    "    v = m1_mid[\"volume\"].resample(rule).sum()\n",
    "    sp = m1_mid[\"spread\"].resample(rule).mean()\n",
    "\n",
    "    out = pd.DataFrame({\"open\":o,\"high\":h,\"low\":l,\"close\":c,\"volume\":v,\"spread\":sp}).dropna()\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# INDICATORS\n",
    "# -------------------------\n",
    "def ema(s, n): \n",
    "    return s.ewm(span=n, adjust=False).mean()\n",
    "\n",
    "def rsi(close, n=14):\n",
    "    d = close.diff()\n",
    "    up = d.clip(lower=0).rolling(n).mean()\n",
    "    dn = (-d.clip(upper=0)).rolling(n).mean()\n",
    "    rs = up / (dn + 1e-12)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def atr(df, n=14):\n",
    "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    prev = close.shift(1)\n",
    "    tr = pd.concat([(high-low), (high-prev).abs(), (low-prev).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def macd(close, fast=12, slow=26, sig=9):\n",
    "    macd_line = ema(close, fast) - ema(close, slow)\n",
    "    sig_line = ema(macd_line, sig)\n",
    "    hist = macd_line - sig_line\n",
    "    return macd_line, sig_line, hist\n",
    "\n",
    "def bollinger(close, n=20, k=2.0):\n",
    "    mid = close.rolling(n).mean()\n",
    "    sd  = close.rolling(n).std()\n",
    "    up  = mid + k*sd\n",
    "    lo  = mid - k*sd\n",
    "    width = (up - lo) / (close.abs() + 1e-12)\n",
    "    z = (close - mid) / (sd + 1e-12)\n",
    "    return mid, up, lo, width, z\n",
    "\n",
    "def rolling_z(x, n=100):\n",
    "    return (x - x.rolling(n).mean()) / (x.rolling(n).std() + 1e-12)\n",
    "\n",
    "# -------------------------\n",
    "# FEATURES (IMPROVED)\n",
    "# -------------------------\n",
    "def build_features(c: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = c.copy()\n",
    "\n",
    "    # returns\n",
    "    df[\"ret1\"]  = df[\"close\"].pct_change(1)\n",
    "    df[\"ret4\"]  = df[\"close\"].pct_change(4)\n",
    "    df[\"ret8\"]  = df[\"close\"].pct_change(8)\n",
    "    df[\"ret16\"] = df[\"close\"].pct_change(16)\n",
    "\n",
    "    # EMAs + slopes\n",
    "    df[\"ema20\"] = ema(df[\"close\"], 20)\n",
    "    df[\"ema50\"] = ema(df[\"close\"], 50)\n",
    "    df[\"ema100\"] = ema(df[\"close\"], 100)\n",
    "\n",
    "    df[\"ema_diff2050\"] = (df[\"ema20\"] - df[\"ema50\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"ema_diff50100\"] = (df[\"ema50\"] - df[\"ema100\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "\n",
    "    df[\"ema20_slope\"] = df[\"ema20\"].diff(3) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"ema50_slope\"] = df[\"ema50\"].diff(3) / (df[\"close\"].abs() + 1e-12)\n",
    "\n",
    "    # RSI + RSI slope\n",
    "    df[\"rsi14\"] = rsi(df[\"close\"], 14)\n",
    "    df[\"rsi_slope\"] = df[\"rsi14\"].diff(3)\n",
    "\n",
    "    # MACD\n",
    "    m, s, h = macd(df[\"close\"], 12, 26, 9)\n",
    "    df[\"macd\"] = m\n",
    "    df[\"macd_sig\"] = s\n",
    "    df[\"macd_hist\"] = h\n",
    "    df[\"macd_cross_up\"] = ((df[\"macd\"].shift(1) <= df[\"macd_sig\"].shift(1)) & (df[\"macd\"] > df[\"macd_sig\"])).astype(int)\n",
    "    df[\"macd_cross_dn\"] = ((df[\"macd\"].shift(1) >= df[\"macd_sig\"].shift(1)) & (df[\"macd\"] < df[\"macd_sig\"])).astype(int)\n",
    "\n",
    "    # ATR + range structure\n",
    "    df[\"atr14\"] = atr(df, 14)\n",
    "    df[\"atr_pct\"] = df[\"atr14\"] / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"atr_z\"] = rolling_z(df[\"atr_pct\"], 200)\n",
    "\n",
    "    df[\"hl_range\"] = (df[\"high\"] - df[\"low\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"oc_range\"] = (df[\"close\"] - df[\"open\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"body_to_range\"] = (df[\"oc_range\"].abs()) / (df[\"hl_range\"] + 1e-12)\n",
    "\n",
    "    # Bollinger regime + position\n",
    "    bb_mid, bb_up, bb_lo, bb_width, bb_z = bollinger(df[\"close\"], 20, 2.0)\n",
    "    df[\"bb_width\"] = bb_width\n",
    "    df[\"bb_z\"] = bb_z\n",
    "    df[\"bb_width_z\"] = rolling_z(df[\"bb_width\"], 200)\n",
    "\n",
    "    # Spread/Volume features\n",
    "    df[\"spread_pct\"] = df[\"spread\"] / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"spread_z\"] = rolling_z(df[\"spread_pct\"], 200)\n",
    "\n",
    "    df[\"vol_z\"] = (df[\"volume\"] - df[\"volume\"].rolling(200).mean()) / (df[\"volume\"].rolling(200).std() + 1e-12)\n",
    "    df[\"vol_chg\"] = df[\"volume\"].pct_change(5).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Time-of-day features (helps FX sometimes)\n",
    "    # index is UTC\n",
    "    hrs = df.index.hour + df.index.minute/60.0\n",
    "    df[\"tod_sin\"] = np.sin(2*np.pi*hrs/24.0)\n",
    "    df[\"tod_cos\"] = np.cos(2*np.pi*hrs/24.0)\n",
    "\n",
    "    # Simple regime flags\n",
    "    df[\"trend_flag\"] = (df[\"ema20\"] > df[\"ema50\"]).astype(int)\n",
    "    df[\"vol_flag\"]   = (df[\"atr_z\"] > 0).astype(int)\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    return df.dropna()\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"ret1\",\"ret4\",\"ret8\",\"ret16\",\n",
    "    \"ema_diff2050\",\"ema_diff50100\",\"ema20_slope\",\"ema50_slope\",\n",
    "    \"rsi14\",\"rsi_slope\",\n",
    "    \"macd\",\"macd_sig\",\"macd_hist\",\"macd_cross_up\",\"macd_cross_dn\",\n",
    "    \"atr_pct\",\"atr_z\",\"hl_range\",\"oc_range\",\"body_to_range\",\n",
    "    \"bb_width\",\"bb_z\",\"bb_width_z\",\n",
    "    \"spread_pct\",\"spread_z\",\n",
    "    \"vol_z\",\"vol_chg\",\n",
    "    \"tod_sin\",\"tod_cos\",\n",
    "    \"trend_flag\",\"vol_flag\",\n",
    "]\n",
    "\n",
    "def make_label(df, H):\n",
    "    fut = df[\"close\"].shift(-H)\n",
    "    return (fut > df[\"close\"]).astype(int)\n",
    "\n",
    "def future_return(df, H):\n",
    "    fut = df[\"close\"].shift(-H)\n",
    "    return (fut - df[\"close\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "\n",
    "def time_split_idx(n, train_frac=0.70, val_frac=0.15):\n",
    "    tr_end = int(n * train_frac)\n",
    "    va_end = int(n * (train_frac + val_frac))\n",
    "    return slice(0,tr_end), slice(tr_end,va_end), slice(va_end,n)\n",
    "\n",
    "# -------------------------\n",
    "# EXPECTANCY + GATING SEARCH\n",
    "# -------------------------\n",
    "def compute_expectancy(p_up, y_true, f_ret, cost, buy_th, sell_th):\n",
    "    \"\"\"\n",
    "    Direction model gating:\n",
    "      BUY if p>=buy_th\n",
    "      SELL if p<=sell_th\n",
    "    PnL proxy per trade:\n",
    "      BUY: +future_return - cost\n",
    "      SELL: -(future_return) - cost\n",
    "    \"\"\"\n",
    "    p_up = np.asarray(p_up)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    f_ret = np.asarray(f_ret).astype(float)\n",
    "    cost = np.asarray(cost).astype(float)\n",
    "\n",
    "    take_buy = p_up >= buy_th\n",
    "    take_sell = p_up <= sell_th\n",
    "    take = take_buy | take_sell\n",
    "\n",
    "    if take.sum() == 0:\n",
    "        return {\n",
    "            \"trades\": 0,\n",
    "            \"win_rate\": 0.0,\n",
    "            \"expectancy\": -np.inf,\n",
    "            \"avg_ret\": 0.0\n",
    "        }\n",
    "\n",
    "    pred_dir = np.where(take_buy, 1, 0)  # 1=UP (BUY), 0=DOWN (SELL)\n",
    "    wins = (pred_dir[take] == y_true[take])\n",
    "    win_rate = float(wins.mean() * 100.0)\n",
    "\n",
    "    dir_sign = np.where(take_buy, 1.0, -1.0)\n",
    "    pnl = dir_sign[take] * f_ret[take] - np.maximum(cost[take], COST_FLOOR)\n",
    "    expectancy = float(np.mean(pnl))\n",
    "    avg_ret = float(np.mean(dir_sign[take] * f_ret[take]))\n",
    "\n",
    "    return {\n",
    "        \"trades\": int(take.sum()),\n",
    "        \"win_rate\": win_rate,\n",
    "        \"expectancy\": expectancy,\n",
    "        \"avg_ret\": avg_ret\n",
    "    }\n",
    "\n",
    "def baseline_stats(p_up, y_true, f_ret, cost):\n",
    "    \"\"\"\n",
    "    Baseline = always trade with p>=0.5 BUY else SELL\n",
    "    \"\"\"\n",
    "    p_up = np.asarray(p_up)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    f_ret = np.asarray(f_ret).astype(float)\n",
    "    cost = np.asarray(cost).astype(float)\n",
    "\n",
    "    take_buy = p_up >= 0.5\n",
    "    pred_dir = np.where(take_buy, 1, 0)\n",
    "    wins = (pred_dir == y_true)\n",
    "    win_rate = float(wins.mean() * 100.0)\n",
    "\n",
    "    dir_sign = np.where(take_buy, 1.0, -1.0)\n",
    "    pnl = dir_sign * f_ret - np.maximum(cost, COST_FLOOR)\n",
    "    expectancy = float(np.mean(pnl))\n",
    "    return {\n",
    "        \"trades\": int(len(y_true)),\n",
    "        \"win_rate\": win_rate,\n",
    "        \"expectancy\": expectancy\n",
    "    }\n",
    "\n",
    "def search_best_thresholds(p_up, y_true, f_ret, cost, tf_label):\n",
    "    base = baseline_stats(p_up, y_true, f_ret, cost)\n",
    "\n",
    "    n = len(y_true)\n",
    "    min_trades = max(int(n * MIN_TRADES_FRAC), 50)\n",
    "    max_trades = int(n * MAX_TRADES_FRAC)\n",
    "\n",
    "    runs = []\n",
    "    for b in BUY_GRID:\n",
    "        for s in SELL_GRID:\n",
    "            if s >= 0.5 or b <= 0.5 or s >= b:\n",
    "                continue\n",
    "            runs.append((float(b), float(s)))\n",
    "\n",
    "    best = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    pbar = tqdm(total=len(runs), desc=f\"{tf_label} threshold search\")\n",
    "    for i, (b, s) in enumerate(runs, 1):\n",
    "        st = compute_expectancy(p_up, y_true, f_ret, cost, b, s)\n",
    "\n",
    "        # constraints\n",
    "        if st[\"trades\"] < min_trades:\n",
    "            score = -np.inf\n",
    "        elif st[\"trades\"] > max_trades:\n",
    "            score = -np.inf\n",
    "        elif st[\"win_rate\"] < (base[\"win_rate\"] + MIN_WIN_LIFT):\n",
    "            score = -np.inf\n",
    "        else:\n",
    "            # objective: maximize expectancy after costs\n",
    "            score = st[\"expectancy\"]\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = {\n",
    "                \"buy\": b, \"sell\": s,\n",
    "                \"stats\": st,\n",
    "                \"baseline\": base,\n",
    "                \"min_trades\": int(min_trades),\n",
    "                \"max_trades\": int(max_trades),\n",
    "            }\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"progress\": f\"{(100*i/len(runs)):.1f}%\",\n",
    "            \"best_exp\": f\"{(best_score if np.isfinite(best_score) else -9):.6f}\",\n",
    "            \"best_pair\": f\"{best['buy']:.2f}/{best['sell']:.2f}\" if best else \"NA\",\n",
    "            \"best_trades\": best[\"stats\"][\"trades\"] if best else 0\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return best\n",
    "\n",
    "# -------------------------\n",
    "# TRAINING\n",
    "# -------------------------\n",
    "def train_tf(tf, H):\n",
    "    log(f\"STEP 1/7: Load CSV for {tf} (H={H})\")\n",
    "    raw = pd.read_csv(CSV_PATH)\n",
    "    raw = ensure_dt_index(raw, \"time\")\n",
    "    mid = mid_ohlc_from_oanda(raw)\n",
    "\n",
    "    log(f\"STEP 2/7: Resample -> {tf}\")\n",
    "    candles = resample_tf(mid, tf)\n",
    "    log(f\"   {tf} candles: {len(candles):,}\")\n",
    "\n",
    "    log(\"STEP 3/7: Features + Labels\")\n",
    "    feats = build_features(candles)\n",
    "    feats[\"y\"] = make_label(feats, H).astype(int)\n",
    "    feats[\"f_ret\"] = future_return(feats, H)\n",
    "    feats = feats.dropna(subset=[\"y\",\"f_ret\"])\n",
    "\n",
    "    # cost proxy: spread at bar t as pct of close\n",
    "    feats[\"cost\"] = np.maximum(feats[\"spread_pct\"].astype(float).values, COST_FLOOR)\n",
    "\n",
    "    X = feats[FEATURE_COLS].astype(float).values\n",
    "    y = feats[\"y\"].values.astype(int)\n",
    "    f_ret = feats[\"f_ret\"].values.astype(float)\n",
    "    cost = feats[\"cost\"].values.astype(float)\n",
    "\n",
    "    tr, va, te = time_split_idx(len(feats))\n",
    "    Xtr, ytr = X[tr], y[tr]\n",
    "    Xva, yva = X[va], y[va]\n",
    "    Xte, yte = X[te], y[te]\n",
    "\n",
    "    fva, cva = f_ret[va], cost[va]\n",
    "    fte, cte = f_ret[te], cost[te]\n",
    "\n",
    "    log(f\"   Split: train={len(ytr):,} val={len(yva):,} test={len(yte):,}\")\n",
    "\n",
    "    # Candidate runs\n",
    "    runs = []\n",
    "    for params in ParameterGrid({\"C\": LOGREG_C}):\n",
    "        runs.append((\"logreg\", params))\n",
    "    for params in ParameterGrid(RF_GRID):\n",
    "        runs.append((\"rf\", params))\n",
    "    for params in ParameterGrid(HGB_GRID):\n",
    "        runs.append((\"hgb\", params))\n",
    "\n",
    "    log(f\"STEP 4/7: Train+Validate (model search: {len(runs)} runs)\")\n",
    "    best_model = None\n",
    "    best_ll = float(\"inf\")\n",
    "    best_desc = None\n",
    "\n",
    "    pbar = tqdm(total=len(runs), desc=f\"{tf} model search\")\n",
    "    for i, (name, params) in enumerate(runs, 1):\n",
    "        if name == \"logreg\":\n",
    "            base = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"clf\", LogisticRegression(\n",
    "                    max_iter=3000, class_weight=\"balanced\", solver=\"lbfgs\",\n",
    "                    C=float(params[\"C\"])\n",
    "                ))\n",
    "            ])\n",
    "        elif name == \"rf\":\n",
    "            base = RandomForestClassifier(\n",
    "                random_state=42, n_jobs=-1, class_weight=\"balanced_subsample\",\n",
    "                n_estimators=int(params[\"n_estimators\"]),\n",
    "                max_depth=int(params[\"max_depth\"]),\n",
    "                min_samples_leaf=int(params[\"min_samples_leaf\"])\n",
    "            )\n",
    "        else:\n",
    "            base = HistGradientBoostingClassifier(\n",
    "                random_state=42,\n",
    "                max_depth=int(params[\"max_depth\"]),\n",
    "                learning_rate=float(params[\"learning_rate\"]),\n",
    "                max_iter=int(params[\"max_iter\"])\n",
    "            )\n",
    "\n",
    "        clf = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
    "        clf.fit(Xtr, ytr)\n",
    "\n",
    "        p_va = clf.predict_proba(Xva)[:, 1]\n",
    "        ll = log_loss(yva, np.clip(p_va, 1e-6, 1-1e-6))\n",
    "\n",
    "        if ll < best_ll:\n",
    "            best_ll = ll\n",
    "            best_model = clf\n",
    "            best_desc = (name, params)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"progress\": f\"{(100*i/len(runs)):.1f}%\",\n",
    "            \"best_ll\": f\"{best_ll:.4f}\",\n",
    "            \"best\": best_desc[0]\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    log(\"STEP 5/7: Threshold search on VALIDATION (maximize expectancy after costs)\")\n",
    "    p_va_best = best_model.predict_proba(Xva)[:, 1]\n",
    "    best_thr = search_best_thresholds(p_va_best, yva, fva, cva, tf_label=tf)\n",
    "\n",
    "    if best_thr is None or not np.isfinite(best_thr[\"stats\"][\"expectancy\"]):\n",
    "        # fallback if constraints too strict\n",
    "        log(\"   WARNING: No threshold pair met constraints. Falling back to buy=0.55 sell=0.45\")\n",
    "        best_thr = {\n",
    "            \"buy\": 0.55, \"sell\": 0.45,\n",
    "            \"stats\": compute_expectancy(p_va_best, yva, fva, cva, 0.55, 0.45),\n",
    "            \"baseline\": baseline_stats(p_va_best, yva, fva, cva),\n",
    "            \"min_trades\": int(max(int(len(yva)*MIN_TRADES_FRAC), 50)),\n",
    "            \"max_trades\": int(len(yva)*MAX_TRADES_FRAC)\n",
    "        }\n",
    "\n",
    "    log(f\"   VAL baseline: win_rate={best_thr['baseline']['win_rate']:.2f}% exp={best_thr['baseline']['expectancy']:.6f}\")\n",
    "    log(f\"   VAL best gate: buy={best_thr['buy']:.2f} sell={best_thr['sell']:.2f} \"\n",
    "        f\"trades={best_thr['stats']['trades']} win_rate={best_thr['stats']['win_rate']:.2f}% \"\n",
    "        f\"exp={best_thr['stats']['expectancy']:.6f}\")\n",
    "\n",
    "    log(\"STEP 6/7: Test metrics + gated performance (using chosen thresholds)\")\n",
    "    p_te = best_model.predict_proba(Xte)[:, 1]\n",
    "\n",
    "    test_ll  = log_loss(yte, np.clip(p_te, 1e-6, 1-1e-6))\n",
    "    test_auc = roc_auc_score(yte, p_te) if len(np.unique(yte)) > 1 else float(\"nan\")\n",
    "    test_acc = accuracy_score(yte, (p_te >= 0.5).astype(int))\n",
    "    cm = confusion_matrix(yte, (p_te >= 0.5).astype(int))\n",
    "    rep = classification_report(yte, (p_te >= 0.5).astype(int), digits=4)\n",
    "\n",
    "    base_te = baseline_stats(p_te, yte, fte, cte)\n",
    "    gate_te = compute_expectancy(p_te, yte, fte, cte, best_thr[\"buy\"], best_thr[\"sell\"])\n",
    "\n",
    "    log(f\"BEST MODEL: {best_desc[0]} params={best_desc[1]}\")\n",
    "    log(f\"VAL LogLoss={best_ll:.4f}\")\n",
    "    log(f\"TEST LogLoss={test_ll:.4f} | AUC={test_auc:.4f} | ACC={test_acc:.4f}\")\n",
    "\n",
    "    log(f\"TEST baseline: trades={base_te['trades']} win_rate={base_te['win_rate']:.2f}% exp={base_te['expectancy']:.6f}\")\n",
    "    log(f\"TEST gated   : buy={best_thr['buy']:.2f} sell={best_thr['sell']:.2f} \"\n",
    "        f\"trades={gate_te['trades']} win_rate={gate_te['win_rate']:.2f}% exp={gate_te['expectancy']:.6f}\")\n",
    "\n",
    "    print(\"\\n--- CONFUSION MATRIX (TEST @ 0.5 cutoff) ---\")\n",
    "    print(cm)\n",
    "    print(\"\\n--- CLASSIFICATION REPORT (TEST @ 0.5 cutoff) ---\")\n",
    "    print(rep)\n",
    "\n",
    "    log(\"STEP 7/7: Save artifacts\")\n",
    "    tag = f\"{tf}_H{H}\"\n",
    "    model_path = os.path.join(OUT_DIR, f\"direction_model_{tag}.joblib\")\n",
    "    meta_path  = os.path.join(OUT_DIR, f\"direction_model_{tag}.json\")\n",
    "\n",
    "    dump(best_model, model_path)\n",
    "\n",
    "    meta = {\n",
    "        \"tf\": tf,\n",
    "        \"horizon_bars\": int(H),\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "\n",
    "        \"best_model\": {\n",
    "            \"name\": best_desc[0],\n",
    "            \"params\": best_desc[1],\n",
    "            \"val_logloss\": float(best_ll),\n",
    "        },\n",
    "\n",
    "        \"threshold_search\": {\n",
    "            \"buy_grid\": BUY_GRID.tolist(),\n",
    "            \"sell_grid\": SELL_GRID.tolist(),\n",
    "            \"min_trades_frac\": float(MIN_TRADES_FRAC),\n",
    "            \"max_trades_frac\": float(MAX_TRADES_FRAC),\n",
    "            \"min_win_lift\": float(MIN_WIN_LIFT),\n",
    "        },\n",
    "\n",
    "        \"thresholds_best\": {\n",
    "            \"buy\": float(best_thr[\"buy\"]),\n",
    "            \"sell\": float(best_thr[\"sell\"]),\n",
    "            \"val\": best_thr[\"stats\"],\n",
    "            \"val_baseline\": best_thr[\"baseline\"],\n",
    "            \"constraints\": {\"min_trades\": int(best_thr[\"min_trades\"]), \"max_trades\": int(best_thr[\"max_trades\"])},\n",
    "        },\n",
    "\n",
    "        \"test\": {\n",
    "            \"logloss\": float(test_ll),\n",
    "            \"auc\": float(test_auc),\n",
    "            \"acc\": float(test_acc),\n",
    "            \"cm\": cm.tolist(),\n",
    "            \"baseline\": base_te,\n",
    "            \"gated\": {\n",
    "                \"buy\": float(best_thr[\"buy\"]),\n",
    "                \"sell\": float(best_thr[\"sell\"]),\n",
    "                \"stats\": gate_te\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    log(f\"Saved model -> {model_path}\")\n",
    "    log(f\"Saved meta  -> {meta_path}\")\n",
    "\n",
    "    return model_path, meta_path\n",
    "\n",
    "# -------------------------\n",
    "# RUN BOTH TFs\n",
    "# -------------------------\n",
    "log(\"=== TRAIN M15 (H=4) ===\")\n",
    "m15_model, m15_meta = train_tf(\"M15\", 4)\n",
    "\n",
    "log(\"=== TRAIN M30 (H=2) ===\")\n",
    "m30_model, m30_meta = train_tf(\"M30\", 2)\n",
    "\n",
    "log(\"DONE ✅\")\n",
    "(m15_model, m30_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299338d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:11:13] === TRAIN M15 (H=4) ===\n",
      "[23:11:13] STEP 1/7: Load CSV\n",
      "[23:11:18] STEP 2/7: Resample -> M15\n",
      "[23:11:18]    M15 candles: 124,543\n",
      "[23:11:18] STEP 3/7: Features + Labels\n",
      "[23:11:19]    Split: train=87,031 val=18,650 test=18,650\n",
      "[23:11:19] STEP 4/7: Train+Validate (model search: 39 runs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a243d7cce664fd782e853ddd9f29184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M15 model search:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:21:07] STEP 5/7: Threshold search on VALIDATION (maximize expectancy after costs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a3c4bf53fe4df8840b6a8b29c19e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Threshold search (VAL):   0%|          | 0/675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:21:08] VAL baseline: trades=18646 win_rate=41.06% exp=-0.000133\n",
      "[23:21:08] VAL best gate: buy=0.54 sell=0.43 trades=1099 win_rate=36.40% exp=-0.000097\n",
      "[23:21:08] STEP 6/7: Test metrics + gated performance\n",
      "[23:21:08] BEST MODEL: rf params={'max_depth': 6, 'min_samples_leaf': 100, 'n_estimators': 600}\n",
      "[23:21:08] VAL LogLoss=0.6915\n",
      "[23:21:08] TEST LogLoss=0.6915 | AUC=0.5286 | ACC=0.5182\n",
      "[23:21:08] TEST baseline: trades=18646 win_rate=41.09% exp=-0.000136\n",
      "[23:21:08] TEST gated   : buy=0.54 sell=0.43 trades=1403 win_rate=41.05% exp=-0.000111\n",
      "\n",
      "--- CONFUSION MATRIX (TEST @ 0.5) ---\n",
      "[[5046 4155]\n",
      " [4830 4619]]\n",
      "\n",
      "--- CLASSIFICATION REPORT (TEST @ 0.5) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5109    0.5484    0.5290      9201\n",
      "           1     0.5264    0.4888    0.5069      9449\n",
      "\n",
      "    accuracy                         0.5182     18650\n",
      "   macro avg     0.5187    0.5186    0.5180     18650\n",
      "weighted avg     0.5188    0.5182    0.5178     18650\n",
      "\n",
      "[23:21:08] STEP 7/7: Save artifacts\n",
      "[23:21:09] Saved model -> artifacts_v2\\direction_model_M15_H4.joblib\n",
      "[23:21:09] Saved meta  -> artifacts_v2\\direction_model_M15_H4.json\n",
      "[23:21:09] === TRAIN M30 (H=2) ===\n",
      "[23:21:09] STEP 1/7: Load CSV\n",
      "[23:21:13] STEP 2/7: Resample -> M30\n",
      "[23:21:14]    M30 candles: 62,278\n",
      "[23:21:14] STEP 3/7: Features + Labels\n",
      "[23:21:14]    Split: train=43,446 val=9,310 test=9,310\n",
      "[23:21:14] STEP 4/7: Train+Validate (model search: 39 runs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a17ef299712475fb7574206223f1398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "M30 model search:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:27:32] STEP 5/7: Threshold search on VALIDATION (maximize expectancy after costs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd9451c7a494c8092e1d24ef099c15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Threshold search (VAL):   0%|          | 0/675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:27:33] VAL baseline: trades=9308 win_rate=40.86% exp=-0.000134\n",
      "[23:27:33] VAL best gate: buy=0.55 sell=0.47 trades=877 win_rate=41.62% exp=-0.000056\n",
      "[23:27:33] STEP 6/7: Test metrics + gated performance\n",
      "[23:27:34] BEST MODEL: rf params={'max_depth': 10, 'min_samples_leaf': 100, 'n_estimators': 300}\n",
      "[23:27:34] VAL LogLoss=0.6916\n",
      "[23:27:34] TEST LogLoss=0.6922 | AUC=0.5253 | ACC=0.5194\n",
      "[23:27:34] TEST baseline: trades=9308 win_rate=41.46% exp=-0.000130\n",
      "[23:27:34] TEST gated   : buy=0.55 sell=0.47 trades=1823 win_rate=40.21% exp=-0.000147\n",
      "\n",
      "--- CONFUSION MATRIX (TEST @ 0.5) ---\n",
      "[[2836 1760]\n",
      " [2714 2000]]\n",
      "\n",
      "--- CLASSIFICATION REPORT (TEST @ 0.5) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5110    0.6171    0.5590      4596\n",
      "           1     0.5319    0.4243    0.4720      4714\n",
      "\n",
      "    accuracy                         0.5194      9310\n",
      "   macro avg     0.5215    0.5207    0.5155      9310\n",
      "weighted avg     0.5216    0.5194    0.5150      9310\n",
      "\n",
      "[23:27:34] STEP 7/7: Save artifacts\n",
      "[23:27:34] Saved model -> artifacts_v2\\direction_model_M30_H2.joblib\n",
      "[23:27:34] Saved meta  -> artifacts_v2\\direction_model_M30_H2.json\n",
      "[23:27:34] DONE ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('artifacts_v2\\\\direction_model_M15_H4.joblib',\n",
       " 'artifacts_v2\\\\direction_model_M30_H2.joblib')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, classification_report, confusion_matrix\n",
    "from joblib import dump\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "CSV_PATH = r\"C:\\Users\\Timothy.Mandingwa\\Desktop\\live_app_rules\\data\\EURUSD_OANDA_M1.csv\"\n",
    "OUT_DIR = \"artifacts_v2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# realistic-ish costs (in price terms) - tune these to match your broker\n",
    "# We'll model cost per trade as: cost = spread_mean * cost_mult\n",
    "COST_MULT = 1.0   # 1.0 means \"pay the spread once\" (conservative enough for gating tests)\n",
    "\n",
    "# threshold search\n",
    "BUY_GRID  = np.linspace(0.50, 0.75, 26)  # 0.50..0.75 step 0.01\n",
    "SELL_GRID = np.linspace(0.25, 0.50, 26)  # 0.25..0.50 step 0.01\n",
    "MIN_TRADES_VAL = 250     # require enough validation trades\n",
    "MIN_TRADES_TEST = 250    # require enough test trades\n",
    "\n",
    "# time split\n",
    "TRAIN_FRAC = 0.70\n",
    "VAL_FRAC   = 0.15\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def log(msg):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "def ensure_dt_index(df, time_col=\"time\"):\n",
    "    if time_col in df.columns:\n",
    "        df[time_col] = pd.to_datetime(df[time_col], utc=True, errors=\"coerce\")\n",
    "        df = df.dropna(subset=[time_col]).set_index(time_col)\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "        df = df[~df.index.isna()]\n",
    "    return df.sort_index()\n",
    "\n",
    "def mid_ohlc_from_oanda(df):\n",
    "    needed = [\"bid_o\",\"bid_h\",\"bid_l\",\"bid_c\",\"ask_o\",\"ask_h\",\"ask_l\",\"ask_c\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    # robust numeric conversion\n",
    "    for c in needed + ([\"volume\"] if \"volume\" in df.columns else []):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    out[\"open\"]  = (df[\"bid_o\"] + df[\"ask_o\"]) / 2.0\n",
    "    out[\"high\"]  = (df[\"bid_h\"] + df[\"ask_h\"]) / 2.0\n",
    "    out[\"low\"]   = (df[\"bid_l\"] + df[\"ask_l\"]) / 2.0\n",
    "    out[\"close\"] = (df[\"bid_c\"] + df[\"ask_c\"]) / 2.0\n",
    "    out[\"spread\"] = (df[\"ask_c\"] - df[\"bid_c\"]).fillna(0.0)\n",
    "    out[\"volume\"] = df[\"volume\"].fillna(0.0) if \"volume\" in df.columns else 0.0\n",
    "    return out.dropna()\n",
    "\n",
    "def resample_tf(m1_mid, tf):\n",
    "    tf = tf.upper().strip()\n",
    "    rule = {\"M15\":\"15min\", \"M30\":\"30min\"}[tf]\n",
    "    o = m1_mid[\"open\"].resample(rule).first()\n",
    "    h = m1_mid[\"high\"].resample(rule).max()\n",
    "    l = m1_mid[\"low\"].resample(rule).min()\n",
    "    c = m1_mid[\"close\"].resample(rule).last()\n",
    "    v = m1_mid[\"volume\"].resample(rule).sum()\n",
    "    sp = m1_mid[\"spread\"].resample(rule).mean()\n",
    "    return pd.DataFrame({\"open\":o,\"high\":h,\"low\":l,\"close\":c,\"volume\":v,\"spread\":sp}).dropna()\n",
    "\n",
    "def ema(s, n): \n",
    "    return s.ewm(span=n, adjust=False).mean()\n",
    "\n",
    "def rsi(close, n=14):\n",
    "    d = close.diff()\n",
    "    up = d.clip(lower=0).rolling(n).mean()\n",
    "    dn = (-d.clip(upper=0)).rolling(n).mean()\n",
    "    rs = up / (dn + 1e-12)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def atr(df, n=14):\n",
    "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    prev = close.shift(1)\n",
    "    tr = pd.concat([(high-low), (high-prev).abs(), (low-prev).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def macd(close, fast=12, slow=26, sig=9):\n",
    "    macd_line = ema(close, fast) - ema(close, slow)\n",
    "    sig_line = ema(macd_line, sig)\n",
    "    hist = macd_line - sig_line\n",
    "    return macd_line, sig_line, hist\n",
    "\n",
    "def rolling_z(x, n):\n",
    "    mu = x.rolling(n).mean()\n",
    "    sd = x.rolling(n).std()\n",
    "    return (x - mu) / (sd + 1e-12)\n",
    "\n",
    "def build_features(c):\n",
    "    df = c.copy()\n",
    "\n",
    "    # returns (multi-lag)\n",
    "    for k in [1,2,3,4,6,8,12]:\n",
    "        df[f\"ret{k}\"] = df[\"close\"].pct_change(k)\n",
    "\n",
    "    # trend & slope\n",
    "    df[\"ema20\"] = ema(df[\"close\"], 20)\n",
    "    df[\"ema50\"] = ema(df[\"close\"], 50)\n",
    "    df[\"ema100\"] = ema(df[\"close\"], 100)\n",
    "    df[\"ema_diff_20_50\"] = (df[\"ema20\"] - df[\"ema50\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"ema_diff_50_100\"] = (df[\"ema50\"] - df[\"ema100\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"ema20_slope\"] = df[\"ema20\"].diff() / (df[\"close\"].abs() + 1e-12)\n",
    "\n",
    "    # mean-reversion position\n",
    "    df[\"z_close_50\"] = rolling_z(df[\"close\"], 50)\n",
    "    df[\"z_ret_50\"] = rolling_z(df[\"ret1\"], 50)\n",
    "\n",
    "    # momentum\n",
    "    df[\"rsi14\"] = rsi(df[\"close\"], 14)\n",
    "    df[\"rsi_slope\"] = df[\"rsi14\"].diff()\n",
    "\n",
    "    m, s, h = macd(df[\"close\"], 12, 26, 9)\n",
    "    df[\"macd\"] = m\n",
    "    df[\"macd_sig\"] = s\n",
    "    df[\"macd_hist\"] = h\n",
    "    df[\"macd_hist_slope\"] = df[\"macd_hist\"].diff()\n",
    "\n",
    "    # volatility regime\n",
    "    df[\"atr14\"] = atr(df, 14)\n",
    "    df[\"atr_pct\"] = df[\"atr14\"] / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"atr_pct_z\"] = rolling_z(df[\"atr_pct\"], 200)\n",
    "\n",
    "    df[\"hl_range\"] = (df[\"high\"] - df[\"low\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"oc_range\"] = (df[\"close\"] - df[\"open\"]) / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"range_z\"] = rolling_z(df[\"hl_range\"], 200)\n",
    "\n",
    "    # spread & volume\n",
    "    df[\"spread_pct\"] = df[\"spread\"] / (df[\"close\"].abs() + 1e-12)\n",
    "    df[\"spread_z\"] = rolling_z(df[\"spread_pct\"], 200)\n",
    "    df[\"vol_z\"] = rolling_z(df[\"volume\"], 200)\n",
    "\n",
    "    # time-of-day features (helps FX sometimes)\n",
    "    idx = df.index\n",
    "    df[\"hour\"] = idx.hour\n",
    "    df[\"dow\"] = idx.dayofweek\n",
    "    df[\"sin_hour\"] = np.sin(2*np.pi*df[\"hour\"]/24.0)\n",
    "    df[\"cos_hour\"] = np.cos(2*np.pi*df[\"hour\"]/24.0)\n",
    "    df[\"sin_dow\"] = np.sin(2*np.pi*df[\"dow\"]/7.0)\n",
    "    df[\"cos_dow\"] = np.cos(2*np.pi*df[\"dow\"]/7.0)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def make_label(df, H):\n",
    "    fut = df[\"close\"].shift(-H)\n",
    "    return (fut > df[\"close\"]).astype(int)\n",
    "\n",
    "def time_split_idx(n, train_frac=TRAIN_FRAC, val_frac=VAL_FRAC):\n",
    "    tr_end = int(n * train_frac)\n",
    "    va_end = int(n * (train_frac + val_frac))\n",
    "    return slice(0,tr_end), slice(tr_end,va_end), slice(va_end,n)\n",
    "\n",
    "def expectancy_after_costs(df_eval, p_up, y_true, H, buy_th, sell_th, cost_mult=COST_MULT):\n",
    "    \"\"\"\n",
    "    Uses future return over H bars as \"trade result\":\n",
    "      BUY:  ret = close[t+H]/close[t] - 1\n",
    "      SELL: ret = -(close[t+H]/close[t] - 1)\n",
    "    Costs: subtract spread_mean * cost_mult (in return terms).\n",
    "    \"\"\"\n",
    "    p_up = np.asarray(p_up)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "\n",
    "    take_buy = p_up >= buy_th\n",
    "    take_sell = p_up <= sell_th\n",
    "    take = take_buy | take_sell\n",
    "    ntr = int(take.sum())\n",
    "    if ntr == 0:\n",
    "        return None\n",
    "\n",
    "    close = df_eval[\"close\"].values\n",
    "    fut = np.roll(close, -H)\n",
    "    raw_ret = (fut / (close + 1e-12)) - 1.0\n",
    "    raw_ret[-H:] = np.nan  # last H have no future\n",
    "\n",
    "    dirn = np.where(take_buy, 1, -1)  # +1 buy, -1 sell\n",
    "    trade_ret = raw_ret * dirn\n",
    "    trade_ret = trade_ret[take]\n",
    "    trade_ret = trade_ret[np.isfinite(trade_ret)]\n",
    "    if trade_ret.size == 0:\n",
    "        return None\n",
    "\n",
    "    # cost in return terms\n",
    "    spread = df_eval[\"spread\"].values\n",
    "    spread_ret = (spread / (close + 1e-12)) * float(cost_mult)\n",
    "    spread_ret = spread_ret[take]\n",
    "    spread_ret = spread_ret[np.isfinite(spread_ret)]\n",
    "    if spread_ret.size != trade_ret.size:\n",
    "        m = min(spread_ret.size, trade_ret.size)\n",
    "        trade_ret = trade_ret[:m]\n",
    "        spread_ret = spread_ret[:m]\n",
    "\n",
    "    net = trade_ret - spread_ret\n",
    "\n",
    "    wins = (net > 0)\n",
    "    win_rate = float(wins.mean() * 100.0)\n",
    "    avg_win = float(net[wins].mean()) if wins.any() else 0.0\n",
    "    avg_loss = float(net[~wins].mean()) if (~wins).any() else 0.0\n",
    "    exp = float(net.mean())\n",
    "    return {\n",
    "        \"trades\": int(net.size),\n",
    "        \"win_rate\": win_rate,\n",
    "        \"avg_win\": avg_win,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"expectancy\": exp,\n",
    "    }\n",
    "\n",
    "def threshold_search(df_val, p_val, y_val, H):\n",
    "    best = None\n",
    "    pairs = [(b,s) for b in BUY_GRID for s in SELL_GRID if b > s]\n",
    "    pbar = tqdm(total=len(pairs), desc=\"Threshold search (VAL)\")\n",
    "    for i, (b, s) in enumerate(pairs, 1):\n",
    "        st = expectancy_after_costs(df_val, p_val, y_val, H, b, s)\n",
    "        if st is None:\n",
    "            pbar.update(1); continue\n",
    "        if st[\"trades\"] < MIN_TRADES_VAL:\n",
    "            pbar.update(1); continue\n",
    "\n",
    "        # objective: maximize expectancy; tie-breaker: higher trades; then higher win_rate\n",
    "        key = (st[\"expectancy\"], st[\"trades\"], st[\"win_rate\"])\n",
    "        if (best is None) or (key > best[\"key\"]):\n",
    "            best = {\"buy\": float(b), \"sell\": float(s), \"stats\": st, \"key\": key}\n",
    "\n",
    "        if best is not None:\n",
    "            pbar.set_postfix({\n",
    "                \"best_exp\": f\"{best['stats']['expectancy']:.6f}\",\n",
    "                \"best_pair\": f\"{best['buy']:.2f}/{best['sell']:.2f}\",\n",
    "                \"trades\": best[\"stats\"][\"trades\"],\n",
    "                \"wr%\": f\"{best['stats']['win_rate']:.1f}\",\n",
    "            })\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return best\n",
    "\n",
    "def train_tf(tf, H):\n",
    "    log(f\"=== TRAIN {tf} (H={H}) ===\")\n",
    "    log(\"STEP 1/7: Load CSV\")\n",
    "    raw = pd.read_csv(CSV_PATH)\n",
    "    raw = ensure_dt_index(raw, \"time\")\n",
    "    mid = mid_ohlc_from_oanda(raw)\n",
    "\n",
    "    log(f\"STEP 2/7: Resample -> {tf}\")\n",
    "    candles = resample_tf(mid, tf)\n",
    "    log(f\"   {tf} candles: {len(candles):,}\")\n",
    "\n",
    "    log(\"STEP 3/7: Features + Labels\")\n",
    "    feats = build_features(candles)\n",
    "    feats[\"y\"] = make_label(feats, H)\n",
    "    feats = feats.dropna(subset=[\"y\"])\n",
    "    feats[\"y\"] = feats[\"y\"].astype(int)\n",
    "\n",
    "    feature_cols = [c for c in feats.columns if c not in (\"y\",)]\n",
    "    X = feats[feature_cols].astype(float).values\n",
    "    y = feats[\"y\"].values\n",
    "\n",
    "    tr, va, te = time_split_idx(len(feats))\n",
    "    Xtr, ytr = X[tr], y[tr]\n",
    "    Xva, yva = X[va], y[va]\n",
    "    Xte, yte = X[te], y[te]\n",
    "\n",
    "    df_tr = feats.iloc[tr]\n",
    "    df_va = feats.iloc[va]\n",
    "    df_te = feats.iloc[te]\n",
    "\n",
    "    log(f\"   Split: train={len(ytr):,} val={len(yva):,} test={len(yte):,}\")\n",
    "\n",
    "    # candidate models (kept tight so it runs)\n",
    "    runs = []\n",
    "    runs += [(\"logreg\", p) for p in ParameterGrid({\"C\":[0.25,0.5,1.0,2.0,4.0]})]\n",
    "    runs += [(\"rf\", p) for p in ParameterGrid({\"n_estimators\":[300,600], \"max_depth\":[6,8,10], \"min_samples_leaf\":[25,50,100]})]\n",
    "    runs += [(\"et\", p) for p in ParameterGrid({\"n_estimators\":[600], \"max_depth\":[8,10], \"min_samples_leaf\":[25,50]})]\n",
    "    runs += [(\"hgb\", p) for p in ParameterGrid({\"max_depth\":[3,6], \"learning_rate\":[0.03,0.05,0.08], \"max_iter\":[200,400]})]\n",
    "\n",
    "    log(f\"STEP 4/7: Train+Validate (model search: {len(runs)} runs)\")\n",
    "    best_model = None\n",
    "    best_ll = float(\"inf\")\n",
    "    best_desc = None\n",
    "\n",
    "    # IMPORTANT: time-aware calibration (reduces leakage)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    pbar = tqdm(total=len(runs), desc=f\"{tf} model search\")\n",
    "    for i, (name, params) in enumerate(runs, 1):\n",
    "        if name == \"logreg\":\n",
    "            base = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"clf\", LogisticRegression(\n",
    "                    max_iter=4000, class_weight=\"balanced\",\n",
    "                    solver=\"lbfgs\", C=float(params[\"C\"])\n",
    "                ))\n",
    "            ])\n",
    "        elif name == \"rf\":\n",
    "            base = RandomForestClassifier(\n",
    "                random_state=SEED, n_jobs=-1, class_weight=\"balanced_subsample\",\n",
    "                n_estimators=int(params[\"n_estimators\"]),\n",
    "                max_depth=int(params[\"max_depth\"]),\n",
    "                min_samples_leaf=int(params[\"min_samples_leaf\"])\n",
    "            )\n",
    "        elif name == \"et\":\n",
    "            base = ExtraTreesClassifier(\n",
    "                random_state=SEED, n_jobs=-1, class_weight=\"balanced\",\n",
    "                n_estimators=int(params[\"n_estimators\"]),\n",
    "                max_depth=int(params[\"max_depth\"]),\n",
    "                min_samples_leaf=int(params[\"min_samples_leaf\"])\n",
    "            )\n",
    "        else:\n",
    "            base = HistGradientBoostingClassifier(\n",
    "                random_state=SEED,\n",
    "                max_depth=int(params[\"max_depth\"]),\n",
    "                learning_rate=float(params[\"learning_rate\"]),\n",
    "                max_iter=int(params[\"max_iter\"])\n",
    "            )\n",
    "\n",
    "        clf = CalibratedClassifierCV(base, method=\"sigmoid\", cv=tscv)\n",
    "        clf.fit(Xtr, ytr)\n",
    "\n",
    "        p_va = clf.predict_proba(Xva)[:, 1]\n",
    "        ll = log_loss(yva, np.clip(p_va, 1e-6, 1-1e-6))\n",
    "\n",
    "        if ll < best_ll:\n",
    "            best_ll = ll\n",
    "            best_model = clf\n",
    "            best_desc = (name, params)\n",
    "\n",
    "        pbar.set_postfix({\"best_ll\": f\"{best_ll:.4f}\", \"best\": best_desc[0]})\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    log(\"STEP 5/7: Threshold search on VALIDATION (maximize expectancy after costs)\")\n",
    "    p_va = best_model.predict_proba(Xva)[:, 1]\n",
    "    best_gate = threshold_search(df_va, p_va, yva, H)\n",
    "\n",
    "    # baseline = trade everything with direction = (p>=0.5) and include costs\n",
    "    baseline = expectancy_after_costs(df_va, p_va, yva, H, buy_th=0.5, sell_th=0.5)  # (everything)\n",
    "    if baseline is None:\n",
    "        baseline = {\"trades\":0, \"win_rate\":0.0, \"avg_win\":0.0, \"avg_loss\":0.0, \"expectancy\":0.0}\n",
    "\n",
    "    log(f\"VAL baseline: trades={baseline['trades']} win_rate={baseline['win_rate']:.2f}% exp={baseline['expectancy']:.6f}\")\n",
    "    if best_gate:\n",
    "        log(f\"VAL best gate: buy={best_gate['buy']:.2f} sell={best_gate['sell']:.2f} \"\n",
    "            f\"trades={best_gate['stats']['trades']} win_rate={best_gate['stats']['win_rate']:.2f}% \"\n",
    "            f\"exp={best_gate['stats']['expectancy']:.6f}\")\n",
    "    else:\n",
    "        log(\"VAL best gate: NONE (could not satisfy min-trades constraint)\")\n",
    "\n",
    "    log(\"STEP 6/7: Test metrics + gated performance\")\n",
    "    p_te = best_model.predict_proba(Xte)[:, 1]\n",
    "    test_ll = log_loss(yte, np.clip(p_te, 1e-6, 1-1e-6))\n",
    "    test_auc = roc_auc_score(yte, p_te) if len(np.unique(yte)) > 1 else float(\"nan\")\n",
    "    test_acc = accuracy_score(yte, (p_te >= 0.5).astype(int))\n",
    "\n",
    "    cm = confusion_matrix(yte, (p_te >= 0.5).astype(int))\n",
    "    rep = classification_report(yte, (p_te >= 0.5).astype(int), digits=4)\n",
    "\n",
    "    test_baseline = expectancy_after_costs(df_te, p_te, yte, H, buy_th=0.5, sell_th=0.5) or {\"trades\":0,\"win_rate\":0,\"avg_win\":0,\"avg_loss\":0,\"expectancy\":0}\n",
    "    test_gated = None\n",
    "    if best_gate:\n",
    "        test_gated = expectancy_after_costs(df_te, p_te, yte, H, best_gate[\"buy\"], best_gate[\"sell\"])\n",
    "        if test_gated and test_gated[\"trades\"] < MIN_TRADES_TEST:\n",
    "            test_gated = None\n",
    "\n",
    "    log(f\"BEST MODEL: {best_desc[0]} params={best_desc[1]}\")\n",
    "    log(f\"VAL LogLoss={best_ll:.4f}\")\n",
    "    log(f\"TEST LogLoss={test_ll:.4f} | AUC={test_auc:.4f} | ACC={test_acc:.4f}\")\n",
    "    log(f\"TEST baseline: trades={test_baseline['trades']} win_rate={test_baseline['win_rate']:.2f}% exp={test_baseline['expectancy']:.6f}\")\n",
    "    if test_gated:\n",
    "        log(f\"TEST gated   : buy={best_gate['buy']:.2f} sell={best_gate['sell']:.2f} \"\n",
    "            f\"trades={test_gated['trades']} win_rate={test_gated['win_rate']:.2f}% exp={test_gated['expectancy']:.6f}\")\n",
    "    else:\n",
    "        log(\"TEST gated   : NONE (insufficient trades or no valid gate)\")\n",
    "\n",
    "    print(\"\\n--- CONFUSION MATRIX (TEST @ 0.5) ---\")\n",
    "    print(cm)\n",
    "    print(\"\\n--- CLASSIFICATION REPORT (TEST @ 0.5) ---\")\n",
    "    print(rep)\n",
    "\n",
    "    log(\"STEP 7/7: Save artifacts\")\n",
    "    tag = f\"{tf}_H{H}\"\n",
    "    model_path = os.path.join(OUT_DIR, f\"direction_model_{tag}.joblib\")\n",
    "    meta_path  = os.path.join(OUT_DIR, f\"direction_model_{tag}.json\")\n",
    "\n",
    "    dump(best_model, model_path)\n",
    "\n",
    "    meta = {\n",
    "        \"tf\": tf,\n",
    "        \"horizon_bars\": int(H),\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"cost_mult\": float(COST_MULT),\n",
    "        \"best_model\": {\"name\": best_desc[0], \"params\": best_desc[1], \"val_logloss\": float(best_ll)},\n",
    "        \"thresholds\": ({\"buy\": best_gate[\"buy\"], \"sell\": best_gate[\"sell\"]} if best_gate else None),\n",
    "        \"val\": {\"baseline\": baseline, \"best_gate\": (best_gate[\"stats\"] if best_gate else None)},\n",
    "        \"test\": {\n",
    "            \"logloss\": float(test_ll), \"auc\": float(test_auc), \"acc\": float(test_acc),\n",
    "            \"cm\": cm.tolist(),\n",
    "            \"baseline\": test_baseline,\n",
    "            \"gated\": test_gated,\n",
    "        }\n",
    "    }\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    log(f\"Saved model -> {model_path}\")\n",
    "    log(f\"Saved meta  -> {meta_path}\")\n",
    "    return model_path, meta_path\n",
    "\n",
    "# Run both TFs (Option B)\n",
    "m15_model, m15_meta = train_tf(\"M15\", 4)\n",
    "m30_model, m30_meta = train_tf(\"M30\", 2)\n",
    "\n",
    "log(\"DONE ✅\")\n",
    "(m15_model, m30_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
